# -*- coding: utf-8 -*-
"""Life_Insurance_Risk  Model using Random Forest Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ao9OHQfVNHNpRn8jn79OPocPMSP3DLmu

## **Business Problem**
An insurance company wants to improve its cash flow forecasting by better predicting the life insurance premium using demographic and basic customer health risk metrics at the time of application.

##**Objective**
Build a machine learning model that can predict the premium for a life insurance based on customer's basic information.

##**Dataset Description**

**Age** : age of applicant (primary beneficiary)\
**Sex** : insurance contractor gender, female, male\
**BMI** : Body mass index is a personâ€™s weight in kilograms divided by the square of height in meters. A high BMI can be an indicator of high body fatness.\
**Children** : Number of children or dependents\
**Smoker** : Smoking usage\
**Region** : Northeast, Southeast, Southwest, Northwest\
**Charges** : Life insurance premium based on the data given (target variable)

##**Import Libraries**
"""

import pandas as pd 
import seaborn as sns 
from matplotlib import pyplot as plt 
import numpy as np 
from sklearn.feature_selection import f_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import pickle

"""##**Upload Data**"""

df = pd.read_csv('https://raw.githubusercontent.com/Hafizah/Insurance_dataset/main/insurance.csv') #read and upload our raw data

"""##**Data Inspection**"""

# look at the first 5 rows of the dataset

df.head()

# inspect the number of rows, coulmns, missing values and the data type

df.info()

# look at the statistical information and distribution of the dataset

df.describe(include='all') # use include all to look at the categorical features too

"""##**Data Wrangling**"""

# convert categorical features into numerical features 

dummy=pd.get_dummies(df[['sex','smoker','region']])

dummy

# combine dummy variables with the original dataset

df2 = pd.concat([df,dummy], axis=1)

df2.head()

# drop unused columns

df2.drop(['sex','smoker','region','sex_male','smoker_no'], axis=1, inplace=True) #drop the variable features that we do not need

# double check all columns are drop correctly

df2.head()

# rename columns 

df2= df2.rename(columns={'sex_female':'gender', 'smoker_yes':'smoker','region_northeast':'northeast', 'region_northwest':'northwest', 'region_southeast':'southeast', 'region_southwest':'southwest'}) # 1-female, 0-male, 1-smoker, 0- non smoker

df2.head(3)

# rearrange columns - target variable is now the last column 

df2=df2[['age','bmi','children','gender','smoker','northeast','northwest','southeast','southwest','charges']]

df2.head(1)

"""##**Exploratory Data Analysis**

####Pearson Correlation
- is used to measure the strength and direction (+/-) of the **linear relationship** between two variables.
- gives a quick idea of the potential usefulness of features.
- only valid for **continuous data**, not appropriate for a **binary response variable**.
- correlation (values in the table) are between **0** (not linearly correlated) and **1** or **-1** (highly correlated).
"""

# heatmap to investigate the realtionship between features and between features and target variable
# "smoker" feature is highly corralated with the target variable

plt.figure(figsize=(12,8))
sns.heatmap(df2.corr(), annot=True);

"""####Scatterplot Matrix
- similar to correlation plot.
- shows all data as a grid or scatter plots of all features and the response variable.
- examine data directly in a **concise format**.
"""

# the scatterplot shows the relationship between numerical features and target class
# As the bmi increases, the charges increases.

sns.set_theme(style='ticks')
sns.pairplot(df,hue='smoker'); #include a pairplot of our x and y variables

# histogram of charges against age 
# applicants under the age of 20 pays more premium 

df2['age'].hist(bins=50, color='orange')
plt.xlabel('age')
plt.ylabel('charges');

# histogram of charges against bmi 

df2['bmi'].hist(bins=50, color='orange')
plt.xlabel('bmi')
plt.ylabel('charges');

# histogram of charges against children

df2['children'].hist(bins= 25, color='orange')
plt.xlabel('children')
plt.ylabel('charges');

# shows that the southeast region charges the highest premium, with a difference in premium of about $3000 compared to northwest and southwest

region_ = df.groupby('region').agg({'charges':np.mean})
region_.plot(kind='bar', colormap='Pastel2');

# 1 --> female, 0 --> male
# show that male pays slightly higher premium than female

gender_ = df2.groupby('gender').agg({'charges':np.mean})
gender_.plot(kind='bar', colormap='crest');

# 1 --> smoker, 0 --> non-smoker
# shows that smoker pays a lot more premium with a difference of >$20000

smoker_ = df2.groupby('smoker').agg({'charges':np.mean})
smoker_.plot(kind='bar', colormap='Spectral');

"""##**Feature Selection**

####ANOVA F-test
- ANOVA stands for "analysis of variance".
- Used to test whether features are associated with a response
"""

# convert series into a list

features = df2.columns.tolist()
features

# pick feature columns and target columns

X = df2[features].iloc[:,:-1].values
y = df2[features].iloc[:,-1].values

# check the shape of both variables are consistent

print(X.shape, y.shape)

# initiate the metrics used to evaluate model

[f_stat, f_p_value] = f_regression(X,y)

# feature selection

df_test = pd.DataFrame({'Feature':features[:-1], 'F-Score':f_stat, 'p-value': f_p_value})
df_test.sort_values('p-value')

"""##**Split Data & Feature Scaling**"""

# initiate train test split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2,random_state=0)

X_train.shape

X_test.shape

# Initiate scaler

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_train

X_test = scaler.transform(X_test)

"""##**Build Model**
######**Linear Regression**
"""

# initiate regression model

regressor = LinearRegression()
lm = regressor.fit(X_train, y_train)
lm_pred = lm.predict(X_test)

accuracy = regressor.score(X_test, y_test)
print('Accuracy of Linear Regression model = '+ str(round(accuracy,3)))

"""######**Random Forest Regression**"""

rf_reg = RandomForestRegressor(n_estimators=10, random_state=4)
rf = rf_reg.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

accuracy = rf_reg.score(X_test, y_test)
print('Accuracy of Random Forest model = '+ str(round(accuracy,3)))

"""##**Save Model**"""

# Save model - in binary mode
filename = 'Life insurance Model.pkl'
pickle.dump(rf_reg, open(filename, 'wb'))

"""##**Conclusion**
Random forest regression model produced better accuracy compared to linear regression model
"""